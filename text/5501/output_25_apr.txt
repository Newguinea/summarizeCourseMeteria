这是个课堂录音，你来说说老师说了什么
page_content="Yeah.  OK, so, uh, I'm going to finish off.  Uh uh, some, uh, material on, uh, randomised testing.  So we had looked at, um uh, we've already, in  fact, looked at, uh, one sort of randomised testing.  So mutation testing is a kind of randomised testing.  Uh, we'll now look at a few more, uh, so  we'll talk about, uh, what random testing is in general.  And we'll look at two specific sorts, uh, property based  testing and fuzzing.  And I'll also mention a couple of sorts of, uh,  system testing, uh, for, uh, so sorts of tests that  apply to whole systems or subsystems that test for things  like the robustness of software or its ability to handle  loads.  So we looked at, uh, input space partitioning, which gives  us, uh, techniques for manually creating test cases for, uh,  anything that we can treat as a function.  Uh, so, uh, it requires, um, a fair bit of  thought.  So we've seen ways of analysing the input domain of  something once we've modelled it as a function dividing, dividing  it up into partitions, potentially dividing those up into sub  partitions, uh, selecting test values from those partitions so often  varying between, uh, what we might think of as fairly  sort of normal sort of values versus, um, border or  edge or degenerate cases.  Uh, and so we, uh and we'll often aim to  get, um, hopefully reasonably thorough, uh, coverage of those partitions.  Uh, so we'll often apply, uh, what we called.  Um uh, we'll try and achieve what we called, uh,  base choice coverage.  Uh, but it's a it's a, uh It's a very  useful technique, and we should do it, but it is  quite intensive.  And another possibility.  This isn't something that we would necessarily do instead of  ISB.  But it's a way of augmenting ISP that gives us  additional confidence in our tests is to create the tests  randomly.  So any sort of technique where we randomly choose elements  from the input domain and use those for test cases  is known as random testing.  So one advantage is that it's often quite easy to  generate test cases.  After all, you're just making them randomly.  It's a little more hard to make sure that they  do tell you something useful.  Another advantage is that random testing lacks human biases, so  computers will, when we ask them to select randomly from  things in the input.  If you ask a human to do it, we'll often  have biases or preconceptions about what input values are normal  or better.  We'll tend to focus on those, whereas computers don't have  their biases and they will select things completely randomly.  So that means that random testing can be quite useful  for ferreting out failures in sort of odd corners of  software.  So let's see an example of randomised testing.  Uh, the first sort of randomised testing we'll look at  is, uh, property based testing, And the name comes from  the idea that we are trying to check properties or  invariants of software.  So let's say that we're trying to apply the idea  of random testing to a method we saw in electrified  collapsed spaces.  So we've got some Java method here.  It takes in a string, and it will look for  spots where there's a bunch of contiguous spaces.  So a bunch of spaces next to each other I  will collapse those down into one single space.  Yeah, OK, OK, so if we generate inputs randomly well,  the obvious problem is how do we know what the  result of our tests should be So let's suppose that  we generate the random string R one z blah, blah,  blah.  How do we know what collapsed spaces should actually give?  The only way we have it knowing is by actually  invoking collapse spaces.  So we don't do that to apply the idea behind  property based testing.  We don't try and predict what the exact result should  be, but we try and come up with laws or  properties, which should always hold between the input and the  output of a function.  So an example of that is suppose we ran collapse  spaces on some arbitrary string, and we discovered that the  output string was longer than the input string.  Should that be possible?  The answer is it should.  Something has gone terribly, terribly wrong.  If we get an output string that's longer than the  input string, so we can actually state that as a  law about collapsed spaces.  The output of collapse spaces should never be longer than  the input.  So the out output, uh, might be the same the  same length if there's no spaces or it might be  shorter if there are multiple adjoining spaces, but it should  never be longer.  Uh, you can think of these as, uh, being sort  of what are called, um, sanity checks sometimes.  So checking that are, uh, the output of our programme  sort of meets, uh, minimal, uh, standards for, uh what  what seems sensible.  Uh, what what are some other laws that we can  come up with for?  For the exact collapse spaces method.  So another one is, um Well, we know that it  should never collapse.  Space should never remove a non space character.  Uh, so what, exactly?  We consider, uh, a space might be more than just  a space character.  It might include tabs, uh, might include new lines.  It'll depend on exactly how our, uh uh, what the  specification for our method says.  But it should, uh, whatever the opposite of spaces are,  it should never remove one of those.  So we could test collapsed spaces by using a slightly  simpler function.  So let's suppose that we write something called strip spaces,  which pulls out spaces completely.  Now, that should be very easy to write.  You just kind of iterate over a string and you  only output to the final string.  Anything that isn't a space.  Um, So, uh, if we can think of, uh, simpler  functions that we have a lot of confidence in.  Uh, it's OK to use those in testing our, uh,  collapsed spaces function so we could run strip spaces on  the input and the output.  Uh, and if that law holds, then the results should  always be identical.  So to do property based testing, Um, each of those  properties We, uh we make it a test so we  could call them something like output never longer than input.  And it never removes non space.  So we name our tests after properties that the collapsed  spaces method should have, um, for each of those tests,  we generate a lot of random strings.  Uh, we might generate them completely randomly, so we could  just generate a random.  Uh, we can find out how many possible Unicode characters  there are.  We can generate a decide on a random length for  a string and just generate random characters.  But we'd probably like to exercise more useful functionality.  We'd probably like to ensure that bits of our collapsed  spaces method are going to be exercised, so we'd like  to have a high chance of containing contiguous spaces.  So one way we could do that is, we could  say we might interleave completely random strings with strings that  are only spaces and then completely random.  So we might say, Give me a, uh, a random  bunch of characters.  Uh, so anything at all, uh, now decide on, uh,  a random number of spaces to generate.  We might come up with four, so add four spaces  on, uh, And now add, uh, a few more randomly,  uh, completely randomly generated, uh, characters again.  So we sort of have a sandwich of, um, uh,  random spaces and completely random again, Um, so, yeah, by  saying that the strings are random, it doesn't mean they  have to be, um, completely random.  We're allowed to try and generate things that will have  more of a chance of exposing failures.  So we generate our random strings, we invoke collapsed spaces  on them.  And in each test, we assert that the law we  stated relating input and output holds, So these laws are  another sort of invariant.  So we've talked about class invariants and system invariants, and  these are another sort of invariant.  Uh, these say however, however, the input to a method  changes, uh, this property should always hold Yeah, so we  get the benefit of random testing, which is it's easy  to generate test cases without needing an actual oracle to  tell us what the expected result was.  Um, so property based testing.  It might sound initially a little odd, but it does  give you a lot more confidence in your methods when  you've run thousands or tens of thousands or hundreds of  thousands of randomly generated strings back and you've checked that  these sort of sensible properties are all hold.  So it's a good, quick and fairly easy way of  checking functions for silly mistakes.  And it also forces you to clarify your thinking about  what the preconditions and post conditions of the function are  so it can result in improvements to the documentation.  Another place that property based testing can come in useful  is for bootstrapping tricky functions.  So sometimes we've got, uh, uh, a function where so  sometimes we've got a function where we have a solution,  but it's not terribly efficient.  So often it will be the case that there is,  um, an easy, straightforward, but not especially efficient way of  doing something and a very clever way, but much harder  to implement.  So here.  I've used it as an example.  The idea of edit distance between two strings.  So this is the idea.  You start with one string, you might need to add  letters, remove letters or change letters to get it to  another string.  And this actually comes in handy.  Uh, in areas like bioinformatics when you're looking at, um  uh, say strings of, uh, DNA.  Um So for functions like that, uh, there's often a  fairly straightforward way of implementing this.  So there's an easy algorithm, which is, uh, you can  usually be very sure that you've got it right, but  it's not very efficient, and there's a more complicated way.  So in this case, the edit distance.  It's called the Levenstein Distance, and there are known algorithms  that are efficient, but they are quite fiddly.  So you want to be sure that you've got them  correct.  So what we can do is we can first of  all rate our easy but slow function, and we can  use" metadata={}
 
 
这是个课堂录音，你来说说老师说了什么
page_content="a random number of spaces to generate.  We might come up with four, so add four spaces  on, uh, And now add, uh, a few more randomly,  uh, completely randomly generated, uh, characters again.  So we sort of have a sandwich of, um, uh,  random spaces and completely random again, Um, so, yeah, by  saying that the strings are random, it doesn't mean they  have to be, um, completely random.  We're allowed to try and generate things that will have  more of a chance of exposing failures.  So we generate our random strings, we invoke collapsed spaces  on them.  And in each test, we assert that the law we  stated relating input and output holds, So these laws are  another sort of invariant.  So we've talked about class invariants and system invariants, and  these are another sort of invariant.  Uh, these say however, however, the input to a method  changes, uh, this property should always hold Yeah, so we  get the benefit of random testing, which is it's easy  to generate test cases without needing an actual oracle to  tell us what the expected result was.  Um, so property based testing.  It might sound initially a little odd, but it does  give you a lot more confidence in your methods when  you've run thousands or tens of thousands or hundreds of  thousands of randomly generated strings back and you've checked that  these sort of sensible properties are all hold.  So it's a good, quick and fairly easy way of  checking functions for silly mistakes.  And it also forces you to clarify your thinking about  what the preconditions and post conditions of the function are  so it can result in improvements to the documentation.  Another place that property based testing can come in useful  is for bootstrapping tricky functions.  So sometimes we've got, uh, uh, a function where so  sometimes we've got a function where we have a solution,  but it's not terribly efficient.  So often it will be the case that there is,  um, an easy, straightforward, but not especially efficient way of  doing something and a very clever way, but much harder  to implement.  So here.  I've used it as an example.  The idea of edit distance between two strings.  So this is the idea.  You start with one string, you might need to add  letters, remove letters or change letters to get it to  another string.  And this actually comes in handy.  Uh, in areas like bioinformatics when you're looking at, um  uh, say strings of, uh, DNA.  Um So for functions like that, uh, there's often a  fairly straightforward way of implementing this.  So there's an easy algorithm, which is, uh, you can  usually be very sure that you've got it right, but  it's not very efficient, and there's a more complicated way.  So in this case, the edit distance.  It's called the Levenstein Distance, and there are known algorithms  that are efficient, but they are quite fiddly.  So you want to be sure that you've got them  correct.  So what we can do is we can first of  all rate our easy but slow function, and we can  use property based testing to check that the following law  holds.  So let's suppose our slow function is called edit distance  and the hopefully faster and better one is called fast  edit distance.  We can apply things to random strings and say the  following rule holds the result of the two functions should  always be the same.  We should always get the same result from fast edit  distance as we did from edit distance again, we probably  want to use this to augment other tests rather than  replace them completely.  So we probably would still try and apply ISP.  But when you've had a property based tester, run your  two functions on hundreds or thousands or hundreds of thousands  of, uh of sequences and, uh, you've established that they  always give the same result.  It does give you a lot more confidence, Uh, that,  um, a lot more confidence, uh, that the two are  actually implementing the same thing.  So what are some examples of property based testing frameworks?  Uh, the original one was called Quick Check, and it  was developed for the Haskill language, and it inspired most  of the others.  So it got adopted into other languages.  I've said a good thing about, uh, testing is often  that, um, useful techniques do get, um, quickly adopted into,  uh, other languages.  Uh, so it got adopted into python.  So there's a A framework hypothesis for python, uh, in  Java.  Uh, there are packages called, uh, quick theories.  And, uh, j quick in JavaScript.  There's the Js verifier package, and there's a whole bunch  more listed on the hypothesis documentation page, and we'll just  look at, uh, well, mostly, uh, quick theories in more  detail.  Mhm.  Thank you.  So, uh, for Java, Um, the main testing framework we've  seen so far in Java is, uh, j unit, uh,  specifically version five of J Unit.  Uh, J Unit five does not directly support property based  testing.  Uh, so the nearest, uh, the nearest that we could  get is using, uh, J units parameterized tests.  So, uh, J units way of implementing the data driven  testing pattern?  Um, so we've seen that you can specify that the  input for a test method in J unit comes from  some other method, so you can specify a method source.  In that method, you could use the Java Util random  class to generate particular random inputs.  So for collapsed spaces, maybe that's generating random strings.  But Java Util random has fairly limited capabilities.  A good property based testing framework will have lots of  methods for generating all sorts of random data of different  sorts and often then vetting or checking that random data,  uh, to see how useful it would be for our  tests.  So we might want to, um, uh, say, you know,  let's generate some random numbers, but, uh, check that they're  even or positive or for strings.  Uh, does this string actually contain some spaces?  Uh, and then we'd apply our laws to it.  So the quick theories framework, uh, can be used with  AJ unit.  So here we've got, uh, uh, an example taken from  the quick theories documentation, Um, so we could write a  normal class and annotate it with the usual J unit  test annotation.  And inside it, we'd be making use of methods imported  from the quick theories libraries.  So to run this sort of code, uh, you'd want  to have the quick theories jar file available, so that  would normally go in a lib directory.  Uh, we have a few imports from that, uh, as  before, we often name our test after the property that  we are trying to establish holds, or that we're more  correctly trying to find counter examples against counter examples for,  um so adding two positive events gives a positive event.  This is just a, uh, a short and silly example.  It's not something we'd actually test, because this is this  is really just testing the plus operator in Java, but  it gives a good, simple example.  So we start off by calling QT for quick theories.  It gives us an object, and we invoke the for  all method because we can We can read this.  So this is what's sometimes called a fluent API.  We can read this almost like an English sentence.  It's saying, for all positive integers twice.  So for one positive integer and another positive integer when  we add them together, the result should always be greater  than zero.  Uh, so, yeah, you know, some sort of simple property  that we think should always hold about, uh, positive integers.  So, uh, inside the for all we've got, um, bits  of code that generate random stuff.  Uh, here after check.  Uh, what we've got is what's called a lambda function.  So it takes in our two, uh, parameters.  I n j.  Our two integers uh, and, uh, has some bullying condition.  And the quick theories library will take care of asserting  that so behind the scenes, it will check that this  is true and throw an exception.  If it's not, uh so most property based testing frameworks  will have the following components.  There will be ways of generating random inputs.  So here this is the integers And so integers gives  us random integers.  All positive is sort of a philtre method that checks  that we are limiting that to positive integers.  If we needed them, we could maybe, uh, limit it  in various ways to, uh, say even integers or odd  integers or only prime integers or something like that.  Um, so we might settle for generating integers.  We might want to philtre them with simple conditions.  So that should say even, uh or even integers or  we might have, uh, much more complex random data that  we want to generate.  So, uh, we might say we want to generate, uh,  instances, random instances of the JPEG class and the, uh,  the JPEG class is supposed to represent a valid JPEG  graphic.  So for those complicated, uh, data structures, much of the  same sort of techniques come in handy that we saw  in, uh when we were creating, uh, generators for grammars.  Uh, so you generate.  You look at the the individual parts of a JPEG  class, so it's got various fields.  Uh, you generate random values for those you combine those,  uh, you ensure that any invariants hold that still have  to hold, and eventually you wind up with a whole  random JPEG object.  Um, so the framework should give you plenty of ways  of generating random inputs.  It should, as always, give you ways of asserting things.  So this is following the same arrange act as search  pattern of testing that we've seen before.  Generators are part of the arrange.  You generate the data you need.  The checker does the assertion.  So quick theories we've seen you can use this check  method here.  You can also combine quick theories with the normal J  unit assert methods that we've seen.  So that's possible, too.  And the last component is what's called a shrinker.  So once a test failure is found, so you found  a case where your test fails.  A good property based testing framework will attempt to shrink  the input into the smallest possible one that reproduces the  error.  So let's sort of see an example of that to  get the" metadata={}
 
 
这是个课堂录音，你来说说老师说了什么
page_content="is what's sometimes called a fluent API.  We can read this almost like an English sentence.  It's saying, for all positive integers twice.  So for one positive integer and another positive integer when  we add them together, the result should always be greater  than zero.  Uh, so, yeah, you know, some sort of simple property  that we think should always hold about, uh, positive integers.  So, uh, inside the for all we've got, um, bits  of code that generate random stuff.  Uh, here after check.  Uh, what we've got is what's called a lambda function.  So it takes in our two, uh, parameters.  I n j.  Our two integers uh, and, uh, has some bullying condition.  And the quick theories library will take care of asserting  that so behind the scenes, it will check that this  is true and throw an exception.  If it's not, uh so most property based testing frameworks  will have the following components.  There will be ways of generating random inputs.  So here this is the integers And so integers gives  us random integers.  All positive is sort of a philtre method that checks  that we are limiting that to positive integers.  If we needed them, we could maybe, uh, limit it  in various ways to, uh, say even integers or odd  integers or only prime integers or something like that.  Um, so we might settle for generating integers.  We might want to philtre them with simple conditions.  So that should say even, uh or even integers or  we might have, uh, much more complex random data that  we want to generate.  So, uh, we might say we want to generate, uh,  instances, random instances of the JPEG class and the, uh,  the JPEG class is supposed to represent a valid JPEG  graphic.  So for those complicated, uh, data structures, much of the  same sort of techniques come in handy that we saw  in, uh when we were creating, uh, generators for grammars.  Uh, so you generate.  You look at the the individual parts of a JPEG  class, so it's got various fields.  Uh, you generate random values for those you combine those,  uh, you ensure that any invariants hold that still have  to hold, and eventually you wind up with a whole  random JPEG object.  Um, so the framework should give you plenty of ways  of generating random inputs.  It should, as always, give you ways of asserting things.  So this is following the same arrange act as search  pattern of testing that we've seen before.  Generators are part of the arrange.  You generate the data you need.  The checker does the assertion.  So quick theories we've seen you can use this check  method here.  You can also combine quick theories with the normal J  unit assert methods that we've seen.  So that's possible, too.  And the last component is what's called a shrinker.  So once a test failure is found, so you found  a case where your test fails.  A good property based testing framework will attempt to shrink  the input into the smallest possible one that reproduces the  error.  So let's sort of see an example of that to  get the rationale for why you do this.  So we might want to test methods for a class  which represents some complicated data structure, like a whole word  document or a JPEG graphic.  Um, if we were doing property based testing, we might  say, What are some of the, uh what are some  properties or laws that we think should hold?  Well, a very common one for file formats like this  is that they should round trip.  So the idea is that we start with a JPEG  object in memory.  We wrote it up to a file, and then we  read the file back in again.  Uh, and what we should get is a new object  which is identical to the original, in fact, so that  our tests will run quickly.  Typically, we don't actually write it out to a file  because accessing the file system is quite slow.  Uh, typically, what we do is there should be some  way of converting our JPEG object into a sequence of  bytes or a long array of bytes, which is what  would be written to the file if we did write  it.  So we convert it to a sequence of bits that  are still held in memory that makes the test faster.  We convert from bits back into an object, and we  should get a new object, which is the same as  the original.  So if we manage to generate a JPEG foot where  this doesn't work so the new object differs from the  original, then something's gone wrong somewhere.  But JPEG files can be pretty big, and the data  structure can be quite complicated.  So it could be tricky to track down exactly what  what bit of the file or the data structure is  causing the failure.  So what a shrinker does is it takes a failing  input, and it makes a smaller version of it.  Uh, so let's say, you know, we've got a JPEG  that represents, um uh, maybe 100 and 20 by 100  and 20 pixel image.  Uh, maybe the shrinker, uh, sort of contracts it in  one dimension slightly.  Uh, we see if that fails again.  In fact, usually what the shrinker will do is it'll  generate, um uh, multiple smaller versions.  We'll test the smaller versions, uh, to see if this  one of them still fails.  And if it fails, we've now got a smaller, simpler  example of something.  Uh, that still fails the test.  And we continue that process until we have the smallest  input that still fails our test.  Um, so for simple data, like maybe integers uh, shrinking  might just be that we, uh uh we use a  smaller in.  Maybe that's something that would work for a string.  It might be that, um, given a string, you might  generate, uh, smaller strings.  Let's say 10 smaller strings by chopping out, um, one  random character.  So you end up with 10 shorter strings?  Uh, that might be a way of of shrinking a  string.  So chopping out a random bit of it for complicated  data structures, you'll generally have to explain to the testing  framework how they should be shrunk.  Exactly.  So what it is that shrinking a word document involves.  So maybe that means removing a paragraph or removing a  line or a character or what it means exactly to  shrink a JPEG.  So you'll give the the testing framework various methods it  can call to try and generate a smaller instance of  these, uh, and ideally, you'll end up with the smallest  simple example that still fails your test.  So that's what shrinkers are for so let's see.  Uh, another example of property based testing.  Uh, let's suppose we've got a reverse method that gives  us the reverse of a string.  Uh, we might have some tests that we've come up  with based on ISP.  Uh, but we'd like to have additional confidence that our  reverse, uh, method is doing the sensible thing.  Are there any general laws we can think of that  relate the input to the output?  Well, one is, if you apply reverse twice, you should  get the original input back again.  So again, this is a bit like a sort of  a sanity check of our reverse method.  Um, we start with some random input string.  We reverse it, we reverse it again, and the end  result should be equal to the original.  If we're doing a very large number of these, so  typically property based testing frameworks will run your tests on  hundreds or thousands or tens of thousands of random inputs.  You might want something even quicker than an equality check.  So another one is the output string should always be  the same length as the input string.  So it's another kind of sanity.  Check if you ever end up with a longer string?  Something's gone terribly wrong.  Uh, and in Java checking that the lengths are equal  is much quicker than checking that the whole strings are  equal.  So you could put that as, uh, an initial test  before this, um, self inverse test.  Thank you.  OK, what about, um uh, things that change state?  So in Java, uh, we have a list interface that  represents things that can be represented as a list.  And like most of the Java collections, that's got a  remove method.  Uh, you pass in an object, remove, and it will  search the list for elements which are equal to that  object using the equals method.  And the first one, if there is one, will be  removed.  Otherwise, the method does nothing in this case to describe  your property.  You'll want to have a way of talking about, uh,  the list before you removed anything and the list after  you removed something.  Uh, so specifically, one, uh, sort of check that you  might make is let's say that we call the ill  before the length of the list before you executed, Remove  and ill after the length of the list.  after?  Uh, well, one invariant that holds is either there was  nothing that matched, in which case, the length before and  the length after should be equal.  Or exactly one element should have been removed, In which  case l after should be l before minus one.  So you could give that invariant a handy name.  This is invariant one, but you'd probably want to call  that something like, uh, remove Never increases length or something.  Uh, so it's a bit similar in spirit to our,  uh, collapse spaces invariant when we said, uh, collapse spaces  should only ever remove things that shouldn't never make something  longer.  Um, so we could write this up as a property  based test.  Something like quick theories would generate thousands of random, random  lists, and we could check that this property always holds.  So, uh, like I said, we definitely should still be  applying the other testing techniques we've seen.  So, uh, input space partitioning.  We should apply that we should come up with various  lists that do or don't contain the element being searched  for, But this is a good way of augmenting it.  Um, so, yeah, knowing that, uh, you're still getting sensible  run results when you run this on, uh, hundreds or  thousands of random inputs gives you more confidence that your  method is correct.  So we identify in variants that always hold we generate  random data.  Uh, and if we've, um, run it on thousands or  hundreds of thousands of things that are in varied always  holds, then we get more confident that our method, uh,  that our hypothesis that this" metadata={}
 
 
这是个课堂录音，你来说说老师说了什么
page_content="property based testing frameworks will run your tests on  hundreds or thousands or tens of thousands of random inputs.  You might want something even quicker than an equality check.  So another one is the output string should always be  the same length as the input string.  So it's another kind of sanity.  Check if you ever end up with a longer string?  Something's gone terribly wrong.  Uh, and in Java checking that the lengths are equal  is much quicker than checking that the whole strings are  equal.  So you could put that as, uh, an initial test  before this, um, self inverse test.  Thank you.  OK, what about, um uh, things that change state?  So in Java, uh, we have a list interface that  represents things that can be represented as a list.  And like most of the Java collections, that's got a  remove method.  Uh, you pass in an object, remove, and it will  search the list for elements which are equal to that  object using the equals method.  And the first one, if there is one, will be  removed.  Otherwise, the method does nothing in this case to describe  your property.  You'll want to have a way of talking about, uh,  the list before you removed anything and the list after  you removed something.  Uh, so specifically, one, uh, sort of check that you  might make is let's say that we call the ill  before the length of the list before you executed, Remove  and ill after the length of the list.  after?  Uh, well, one invariant that holds is either there was  nothing that matched, in which case, the length before and  the length after should be equal.  Or exactly one element should have been removed, In which  case l after should be l before minus one.  So you could give that invariant a handy name.  This is invariant one, but you'd probably want to call  that something like, uh, remove Never increases length or something.  Uh, so it's a bit similar in spirit to our,  uh, collapse spaces invariant when we said, uh, collapse spaces  should only ever remove things that shouldn't never make something  longer.  Um, so we could write this up as a property  based test.  Something like quick theories would generate thousands of random, random  lists, and we could check that this property always holds.  So, uh, like I said, we definitely should still be  applying the other testing techniques we've seen.  So, uh, input space partitioning.  We should apply that we should come up with various  lists that do or don't contain the element being searched  for, But this is a good way of augmenting it.  Um, so, yeah, knowing that, uh, you're still getting sensible  run results when you run this on, uh, hundreds or  thousands of random inputs gives you more confidence that your  method is correct.  So we identify in variants that always hold we generate  random data.  Uh, and if we've, um, run it on thousands or  hundreds of thousands of things that are in varied always  holds, then we get more confident that our method, uh,  that our hypothesis that this property holds is true.  Uh, we we can't always be certain.  So it could be that we've, uh, failed to generate,  um uh, a test case that would reveal some particular  fault, but, um, our confidence definitely improves.  All right, so I said we'd look at property based  testing and fuzzing, but I will mention another use of  random testing.  Random testing can be very useful when we want to  regenerate large volumes of data.  So valid or invalid data.  And one place where we might use those is in  various sorts of system testing.  So we'll look at system testing in more detail soon.  Uh, but, uh, a few sorts of system test are,  uh, load testing.  So we might want to know.  How does our software perform under high loads.  Uh, so the, uh, the largest volumes of data we  expect to receive.  So we're we're trying to ensure that our software correctly  handles.  Um uh, the expected load.  Uh, we can do stress testing.  Stress testing asks How does our software behave when we  exceed the expected maximum?  So for stress testing, we're not necessarily expecting that our  system will give us the exact same results that it  should under a normal load.  Eventually.  Eventually, that has to be impossible.  You have to run out of resources.  But what you want is you want that the system  should degrade gracefully.  So rather than users just saying, uh, say browser errors,  saying that the website is down or, uh, time out  so that the, uh, the website is never ended.  Uh, you've often got sort some sort of minimal functionality  that, uh, the system should still be able to perform,  even though it can't do everything.  So if you've got something like a, uh, a web  based storefront, something like Amazon, Uh, if you've got far  more users than you typically expect, uh, you might not  be able to give them all full functionality.  Uh, but you should be able to give them some  functionality.  So maybe you can say we can't put through purchases  now, but you can, uh, save, uh, save the things  you want to buy to a list or a shopping  cart.  So stress testing.  How does our software behave when we exceed the expected  maximum load and lastly, robustness testing, which is how well  does our system handle malformed inputs?  So again, we don't expect to get exactly correct results.  If you feed invalid input to the system, then obviously  it's not going to give you magically give you valid  output.  But there's still minimal things that we expect it not  to do so we typically expect our programme not to  crash and not to have a sink fault.  So all of these things are apply.  All of these things are sorts of testing properties that  apply to a programme or system as a whole, or  maybe to large portions of the programme, but they typically  don't necessarily apply to individual bits.  So at the unit testing scale, um, this robustness, how  well does our system handle malformed inputs at the unit  testing level?  Our methods will have very precise specifications, saying here are  the preconditions of this method.  You're just not allowed to give us malformed input.  It's a requirement that the input be good, because at  that level, our code is going to be called by  other developers.  But at some point, uh, your programme is going to  be viewed or called by end users, and we can't  guarantee that they'll give sensible inputs to our programme.  And we can't always guarantee how many of them will  try and access something at the same time.  And so at a high level, at a system level,  uh, we do want these sorts of properties, uh, so  these would be defined by requirements.  So, um, our requirements would say what the expected load  is.  Our requirements would say when we when we exceed the  expected load.  What are some of the sorts of functionality that the  system should still have?  Uh, and, uh, when we have, uh, malformed input to  our system, Uh, what should it do?  So, ideally, it should print uh uh, no error messages  that the user can understand, uh, and not simply, uh,  se fault or crash.  So it's certainly been the case for me that I've  had say, things on phone apps that have crashed on  starting.  Uh, there have been cases where software like Microsoft Word.  Uh, it's actually been quite a while since I've used  that on, uh, very large documents.  Uh, it certainly used to regularly crash on very large  inputs.  Uh, and, um, that's a pretty bad look for, uh,  customers.  Uh, if you have encountered, uh, software that, um crashed  or failed to start or repeatedly restarted, uh, then that  software probably could have benefited from doing load testing, stress  testing or robustness testing.  OK, the last sort of randomised testing that we'll look  at is called, uh, fuzzing, Uh, so this is often  classified as a kind of robustness testing.  Um, and it's the idea that you, uh, try and  give a programme bad input and you try and crash  the programme.  So we start with the programme under test.  Uh, let's say that we've got a programme that converts,  uh, p n g graphic files into, uh, b MP  s or bitmap files.  Uh, we give it, uh, invalid or unexpected data, and  we monitor the programme, uh, for various sorts of bad  behaviour.  So bad behaviour.  Uh, crashing.  Uh, so where the, uh the programme has a sig  fault.  Uh, that's definitely bad behaviour.  Um, exceptions reaching the end user.  So, in languages, with exceptions like Java in general, your  end users are not going to understand an exception and  a long stack trace in order for your users to  be able to handle bugs in the programme.  What you normally want is some sort of a nice  message saying that an error has occurred and telling them  who to contact so that they can report that.  So if exceptions reach the end user, that's generally regarded  as pretty bad behaviour.  Uh, and, um, if you've got embedded assertions so in  variants So we've talked about, uh, the fact that most  languages have an assert, uh, functionality of some sort.  So Python does C does.  Java has one, but it's unfortunately a bit broken.  Um, So if you've got various sorts of, uh, statements  that always should be true embedded at various spots in  your programme, then that's, um, usually a good thing.  And if one of those ever becomes false, uh, actually,  what will normally happen is the programme will crash.  So if any one of those bad things happen.  Uh, then your programme isn't handling the input data robustly.  Uh, there's a few other things you might like to  monitor for as well.  So a few other sorts of bad behaviour that could  be specified, uh, one is what's called memory leaking.  So this is where as a programme runs for longer  and longer, Um, a well behaved programme should always get  rid of resources and memory that it doesn't need less  well behaved programmes will often keep using more and more  memory the longer they run and lead to a programme  hogging a computer's available memory.  So when that happens, we have what's called a memory  leak." metadata={}
 
 
这是个课堂录音，你来说说老师说了什么
page_content="not simply, uh,  se fault or crash.  So it's certainly been the case for me that I've  had say, things on phone apps that have crashed on  starting.  Uh, there have been cases where software like Microsoft Word.  Uh, it's actually been quite a while since I've used  that on, uh, very large documents.  Uh, it certainly used to regularly crash on very large  inputs.  Uh, and, um, that's a pretty bad look for, uh,  customers.  Uh, if you have encountered, uh, software that, um crashed  or failed to start or repeatedly restarted, uh, then that  software probably could have benefited from doing load testing, stress  testing or robustness testing.  OK, the last sort of randomised testing that we'll look  at is called, uh, fuzzing, Uh, so this is often  classified as a kind of robustness testing.  Um, and it's the idea that you, uh, try and  give a programme bad input and you try and crash  the programme.  So we start with the programme under test.  Uh, let's say that we've got a programme that converts,  uh, p n g graphic files into, uh, b MP  s or bitmap files.  Uh, we give it, uh, invalid or unexpected data, and  we monitor the programme, uh, for various sorts of bad  behaviour.  So bad behaviour.  Uh, crashing.  Uh, so where the, uh the programme has a sig  fault.  Uh, that's definitely bad behaviour.  Um, exceptions reaching the end user.  So, in languages, with exceptions like Java in general, your  end users are not going to understand an exception and  a long stack trace in order for your users to  be able to handle bugs in the programme.  What you normally want is some sort of a nice  message saying that an error has occurred and telling them  who to contact so that they can report that.  So if exceptions reach the end user, that's generally regarded  as pretty bad behaviour.  Uh, and, um, if you've got embedded assertions so in  variants So we've talked about, uh, the fact that most  languages have an assert, uh, functionality of some sort.  So Python does C does.  Java has one, but it's unfortunately a bit broken.  Um, So if you've got various sorts of, uh, statements  that always should be true embedded at various spots in  your programme, then that's, um, usually a good thing.  And if one of those ever becomes false, uh, actually,  what will normally happen is the programme will crash.  So if any one of those bad things happen.  Uh, then your programme isn't handling the input data robustly.  Uh, there's a few other things you might like to  monitor for as well.  So a few other sorts of bad behaviour that could  be specified, uh, one is what's called memory leaking.  So this is where as a programme runs for longer  and longer, Um, a well behaved programme should always get  rid of resources and memory that it doesn't need less  well behaved programmes will often keep using more and more  memory the longer they run and lead to a programme  hogging a computer's available memory.  So when that happens, we have what's called a memory  leak.  And there's ways of inserting code into the executable that  will check for memory leaks and automatically crash the programme  when they occur.  So the general idea, uh, we feed bad I kind  of data into the programme and we we try to,  uh, get it to crash or throw exceptions or fail  in some way.  Now, in the simplest case, we could just try and  feed it completely random data.  So if you've got say something that reads PNG graphic  files, you could just generate random sequences of bites on  desk, feed them to the file and see what happens.  But that's actually really effective because completely random data doesn't  look at all like a PNG file.  And it often doesn't trigger very interesting execution parts so  often completely invalid data will be detected as invalid early  on, and the programme will reject it and behave robustly  and gracefully.  What we want is we want to try and get  things that are close to good inputs, but bad in  some way.  So the idea of mutation that we saw before in  looking at syntax and grammars comes in handy here.  We start with often actually some valid files.  So, uh, we might start with, um uh, in the  case, we were looking at valid P N G files  or, uh, whatever the programme that we're using takes and  randomly alter those to produce new input.  So that's more likely to execute interesting code.  So we can repeatedly mutate our inputs to produce more  and more generations of the input files.  And this is handy for getting the input past any  initials in tax checks.  So it's something that looks mostly correct, and, uh, hopefully  the programme at least starts to process it, but it  might be able to trigger a flaw somewhere.  Um, so mutation based fuzzes are quite handy.  They don't have any sort of particularly deep understanding of  how a JPEG file is constructed so often they just  randomly insert or delete things from the valid JPEG files  to produce invalid ones.  But they still often get quite good results.  Uh, we could try and, uh, give our fuzzes a  bit more information about the input structure it's working with.  So tell it if you've got one valid JPEG file,  here are ways of generating other valid JPEG files.  Uh, you often end up providing the fuzz with rules  that are a bit like the grammars we've seen, uh,  in syntax based testing, uh, that.  Tell it what makes up a JPEG file.  Uh, so those are sometimes called smart versus dumb fuzzes.  The dumb fuzzes, uh, don't have any understanding of the  input.  They just make random changes.  The smart ones do have some understanding of the input,  and, uh, hopefully we'll generate, uh, more interesting inputs.  Uh, you can also classify fuzzes as being white or  grey or black box.  Uh, in the simple case, uh, So what's called Black  Box?  Fuzzing?  Uh, we might monitor a programme for crashes, so we  treat it as a black box in that we shove  stuff into the programme.  Uh, and we checked to see Did it crash?  Uh, but we don't know anything about, uh the, uh,  internal structure of that programme or what bits of it  were Run.  Uh, it's just a black box to us.  One thing we'd often like is for our fuzz to  try and get good code coverage of the programme, though  we'd like it to try and execute lots of different  paths through the programme and lots of different functions.  So white and grey box fuzzes will analyse or instrument  the code of the programme under test.  So the lab that looks at Jaco gives an example  of instrumenting you compile your code.  Uh, you can think of it as compiling your code  with them so that each statement has little counts attached  that say how many times that statement was executed and  writing grey box fusses will try and, uh, observe what  the effect of changing an input is and they'll try  and favour inputs that take the programme down execution paths  it hasn't seen before.  Uh, so a few examples of fuzzes, uh, some of  these, uh, the the earliest fuzzes were written for C  and C plus plus.  So a f l stands for American Fuzzy Lock.  A type of rabbit.  Uh, and that was one of the early fuzzes.  Uh, that works with C and C plus plus programmes  is usually best to, but you usually get best results.  Uh, when the fuzzy you're using is specifically designed for  the language you're working with, Uh, Hong fuzz is another  that works with, um uh, c and C plus plus.  So it's evolutionary.  It, um, keeps track of multiple generations of, uh, inputs.  Uh, it's feedback driven.  So it favours things that, uh, it's found have caused  crashes.  Uh, and it, uh, can check code coverage.  So it's got quite a lot of features.  Uh, and there's a couple of Java fuzzing libraries like  Java fuzz and J Q F.  So an advantage of fuzz testing is it's quite simple  to do.  You just, um So the simplest fuzzes we've seen you  just give them some, uh, some valid inputs.  Uh, you say, here's where my programme is.  and you stop them going and then just leave them.  Uh, so often you'll leave the fuzz running for several  days.  Uh, and at the end of a few days, you'll  check back and see, uh, if it's managed to crash  your programme at all, it's pretty cheap.  It just requires, um so you can often just run  it in the background of, uh, an existing computer.  It doesn't require, um, extensive computational resources.  It tends to be cheaper than, uh, mutation testing.  Uh, because it's just running the programme as normal.  Uh, mutation testing was running your, uh, entire suite of  programmes.  A suite of tests again and again and again.  Uh uh, typically, uh, fuzz testing is, uh, a little,  uh, less intensive when it comes to resources, and it's  actually surprisingly effective.  So it's quite good at finding security flaws in memory.  Unsafe languages like, uh, C N c plus plus.  So in the forum a few weeks ago, I posted  about, uh, a company that had used fuzz testing to  find, uh, flaws in, uh uh, various libraries.  So those are the sorts of testing that we've looked  at.  So they're all examples of random testing we've seen property  based testing.  We've looked a bit at fuzzing.  Uh, and we've noted that you can use random testing  for various sorts of system tests, namely load testing, stress  testing and robustness testing." metadata={}
 
 
